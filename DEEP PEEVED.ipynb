{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPLY PEEVED: Neural Nets for Volcano Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from sklearn import ensemble as ml_models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "\n",
    "\n",
    "class BaseEarthquakes(data.Dataset):\n",
    "    \"\"\"Earthquake and Eruption Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, root, eruption_csv_path, eq_csv_path, split):\n",
    "        self.root  = root\n",
    "        self.split = split\n",
    "        self.eruption_csv_path = eruption_csv_path\n",
    "        self.eq_csv_path = eq_csv_path\n",
    "        self._load_data()\n",
    "        self._normalize()\n",
    "    \n",
    "    def _normalize(self):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.x)\n",
    "        self.x = scaler.transform(self.x)\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Create data list via train, val split\n",
    "        p = PuuOo(eruption_csv_path)\n",
    "        time, lat, lon, depth, mag = load_puuoo_eqs(eq_csv_path)\n",
    "        \n",
    "        if self.split in [\"train\", \"val\"]:\n",
    "            random.seed(0)\n",
    "            percent_train = 0.8 \n",
    "            \n",
    "            # Make additional array for erupting or not\n",
    "            erupt = np.array([p.was_erupting(t) for t in time])\n",
    "            \n",
    "            # Get indices of eruption and non-eruption earthquakes so we can split both\n",
    "            eruption_idx    = [i for i, e in enumerate(erupt) if e == True]\n",
    "            no_eruption_idx = [i for i, e in enumerate(erupt) if e == False]\n",
    "\n",
    "            num_train_eruptions = int(percent_train * len(eruption_idx))\n",
    "            num_val_eruptions   = len(eruption_idx) - num_train_eruptions\n",
    "\n",
    "            num_train_no_eruptions = int(percent_train * len(no_eruption_idx))\n",
    "            num_val_no_eruptions   = len(no_eruption_idx) - num_train_eruptions\n",
    "\n",
    "            train_idx = sorted(random.sample(eruption_idx, num_train_eruptions))\n",
    "            val_idx   = sorted(list(set(eruption_idx) - set(train_idx)))\n",
    "            train_idx += sorted(random.sample(no_eruption_idx, num_train_no_eruptions))\n",
    "            val_idx   += sorted(list(set(no_eruption_idx) - set(train_idx)))\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                idx = train_idx\n",
    "            elif self.split == \"val\":\n",
    "                idx = val_idx\n",
    "            \n",
    "            # Shuffle for data loader\n",
    "            random.shuffle(idx)\n",
    "            \n",
    "            self.time = np.array(time)[idx]\n",
    "            self.lat = np.array(lat)[idx]\n",
    "            self.lon = np.array(lon)[idx]\n",
    "            self.depth = np.array(depth)[idx]\n",
    "            self.mag = np.array(mag)[idx]\n",
    "            self.erupt = np.array(erupt)[idx]\n",
    "            \n",
    "            self.y = self.erupt\n",
    "            self.x = np.array([self.lat, self.lon, \\\n",
    "                               self.depth, self.mag]).T\n",
    "                   \n",
    "        else:\n",
    "            raise ValueError(\"Invalid split name: {}\".format(self.split))\n",
    "\n",
    "    def _get_label_weights(self):\n",
    "        # Get weights for a given dataset\n",
    "        num_erupt = np.sum(self.y)\n",
    "        total = len(self.y)\n",
    "        weights = [1, total/num_erupt]\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.erupt)\n",
    "\n",
    "\n",
    "class NoDerivedFeatures(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoDerivedFeatures, self).__init__(**kwargs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruption_csv_path = 'PuuOo.csv'\n",
    "eq_csv_path       = 'puuoo_earthquakes.csv' \n",
    "\n",
    "dataset_train = NoDerivedFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "dataset_val = NoDerivedFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=50)\n",
    "loader_val = DataLoader(dataset_val, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_layer_model(input_features, hidden_layer_sizes=[1000,500], output_size=2):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1],output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return model.double()\n",
    "\n",
    "def get_four_layer_model(input_features, hidden_layer_sizes=[1000,1000,1000,1000], output_size=2):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[2], hidden_layer_sizes[3]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[3],output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of Sequential(\n",
      "  (0): Linear(in_features=4, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  (9): Sigmoid()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = get_four_layer_model(4)\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Accuracy on ' + loader.dataset.split + ': Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    \n",
    "    weights = loader_train.dataset._get_label_weights()\n",
    "    weights = torch.tensor(weights)\n",
    "    print(weights)\n",
    "    #criterion = nn.CrossEntropyLoss(weight=weights.double())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = criterion(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Epoch %d, loss = %.4f' % (e, loss.item()))\n",
    "            check_accuracy(loader_train, model)\n",
    "            check_accuracy(loader_val, model)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive model: 0.1164449585502217\n",
      "tensor([1.0000, 8.5877])\n",
      "Epoch 0, loss = 0.6975\n",
      "Accuracy on train: Got 2275 / 5187 correct (43.86)\n",
      "Accuracy on val: Got 595 / 1298 correct (45.84)\n",
      "\n",
      "Epoch 10, loss = 0.6641\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 20, loss = 0.6352\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 30, loss = 0.6103\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 40, loss = 0.5891\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 50, loss = 0.5714\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 60, loss = 0.5568\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 70, loss = 0.5449\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 80, loss = 0.5352\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n",
      "Epoch 90, loss = 0.5273\n",
      "Accuracy on train: Got 4583 / 5187 correct (88.36)\n",
      "Accuracy on val: Got 1146 / 1298 correct (88.29)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "print_every = 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "\n",
    "model = get_two_layer_model(4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8835550414497783\n",
      "0.8828967642526965\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(1-loader_train.dataset.y)/len(loader_train.dataset.y))\n",
    "print(np.sum(1-loader_val.dataset.y)/len(loader_val.dataset.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
