{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPLY PEEVED: Neural Nets for Volcano Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from sklearn import ensemble as ml_models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "\n",
    "\n",
    "class BaseEarthquakes(data.Dataset):\n",
    "    \"\"\"Earthquake and Eruption Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, root, eruption_csv_path, eq_csv_path, split):\n",
    "        self.root  = root\n",
    "        self.split = split\n",
    "        self.eruption_csv_path = eruption_csv_path\n",
    "        self.eq_csv_path = eq_csv_path\n",
    "        self._load_data()\n",
    "        self._normalize()\n",
    "    \n",
    "    def _normalize(self):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.x)\n",
    "        self.x = scaler.transform(self.x)\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Create data list via train, val split\n",
    "        p = PuuOo(eruption_csv_path)\n",
    "        time, lat, lon, depth, mag = load_puuoo_eqs(eq_csv_path)\n",
    "        \n",
    "        if self.split in [\"train\", \"val\"]:\n",
    "            random.seed(0)\n",
    "            percent_train = 0.8 \n",
    "            \n",
    "            # Make additional array for erupting or not\n",
    "            erupt = np.array([p.was_erupting(t) for t in time])\n",
    "            \n",
    "            # Get indices of eruption and non-eruption earthquakes so we can split both\n",
    "            eruption_idx    = [i for i, e in enumerate(erupt) if e == True]\n",
    "            no_eruption_idx = [i for i, e in enumerate(erupt) if e == False]\n",
    "\n",
    "            num_train_eruptions = int(percent_train * len(eruption_idx))\n",
    "            num_val_eruptions   = len(eruption_idx) - num_train_eruptions\n",
    "\n",
    "            num_train_no_eruptions = int(percent_train * len(no_eruption_idx))\n",
    "            num_val_no_eruptions   = len(no_eruption_idx) - num_train_eruptions\n",
    "\n",
    "            train_idx = sorted(random.sample(eruption_idx, num_train_eruptions))\n",
    "            val_idx   = sorted(list(set(eruption_idx) - set(train_idx)))\n",
    "            train_idx += sorted(random.sample(no_eruption_idx, num_train_no_eruptions))\n",
    "            val_idx   += sorted(list(set(no_eruption_idx) - set(train_idx)))\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                idx = train_idx\n",
    "            elif self.split == \"val\":\n",
    "                idx = val_idx\n",
    "            \n",
    "            # Shuffle for data loader\n",
    "            random.shuffle(idx)\n",
    "            \n",
    "            self.time = np.array(time)[idx]\n",
    "            self.lat = np.array(lat)[idx]\n",
    "            self.lon = np.array(lon)[idx]\n",
    "            self.depth = np.array(depth)[idx]\n",
    "            self.mag = np.array(mag)[idx]\n",
    "            self.erupt = np.array(erupt)[idx]\n",
    "            \n",
    "            self.y = self.erupt\n",
    "            self.x = np.array([self.lat, self.lon, \\\n",
    "                               self.depth, self.mag]).T\n",
    "                   \n",
    "        else:\n",
    "            raise ValueError(\"Invalid split name: {}\".format(self.split))\n",
    "\n",
    "    def _get_label_weights(self):\n",
    "        # Get weights for a given dataset\n",
    "        num_erupt = np.sum(self.y)\n",
    "        total = len(self.y)\n",
    "        weights = [1, total/num_erupt]\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.erupt)\n",
    "\n",
    "\n",
    "class NoDerivedFeatures(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoDerivedFeatures, self).__init__(**kwargs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "class ExtraFeatures(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoDerivedFeatures, self).__init__(**kwargs)\n",
    "        self.derive_extra_features()\n",
    "    \n",
    "    def derive_extra_features(self):\n",
    "        pass\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruption_csv_path = 'PuuOo.csv'\n",
    "eq_csv_path       = 'puuoo_earthquakes.csv' \n",
    "\n",
    "dataset_train = NoDerivedFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "dataset_val = NoDerivedFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=50)\n",
    "loader_val = DataLoader(dataset_val, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_layer_model(input_features, hidden_layer_sizes=[1000,500], output_size=1):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1],output_size)\n",
    "    )\n",
    "    \n",
    "    return model.double()\n",
    "\n",
    "def get_four_layer_model(input_features, hidden_layer_sizes=[1000,1000,1000,1000], output_size=1):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[2], hidden_layer_sizes[3]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[3],output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of Sequential(\n",
      "  (0): Linear(in_features=4, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (9): Sigmoid()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = get_four_layer_model(4)\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Accuracy on ' + loader.dataset.split + ': Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    \n",
    "    weights = loader_train.dataset._get_label_weights()\n",
    "    weights = torch.tensor(weights)\n",
    "    print('Weghts are: ', weights)\n",
    "    #criterion = nn.CrossEntropyLoss(weight=weights.double())\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=weights[1])\n",
    "\n",
    "    criterion.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            #y = torch.unsqueeze(y,1)\n",
    "           \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "                    \n",
    "            scores = model(x)\n",
    "            scores = torch.squeeze(scores)\n",
    "            loss = criterion(scores, y.float())\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Epoch %d, loss = %.4f' % (e, loss.item()))\n",
    "            check_accuracy(loader_train, model)\n",
    "            check_accuracy(loader_val, model)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weghts are:  tensor([1.0000, 5.2500])\n",
      "Epoch 0, loss = 1.4690\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n",
      "Epoch 10, loss = 1.4261\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00005\n",
    "print_every = 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "\n",
    "model = get_two_layer_model(4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n",
      "0.8089368258859785\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(1-loader_train.dataset.y)/len(loader_train.dataset.y))\n",
    "print(np.sum(1-loader_val.dataset.y)/len(loader_val.dataset.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weghts are:  tensor([1.0000, 5.2500])\n",
      "Epoch 0, loss = 0.6911\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n",
      "Epoch 10, loss = 0.6904\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n",
      "Epoch 20, loss = 0.6898\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n",
      "Epoch 30, loss = 0.6892\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n",
      "Epoch 40, loss = 0.6886\n",
      "Accuracy on train: Got 4199 / 5187 correct (80.95)\n",
      "Accuracy on val: Got 1050 / 1298 correct (80.89)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-19f41859ec3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-43c1b3929ae8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs229/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs229/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00002\n",
    "print_every = 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "\n",
    "model = get_four_layer_model(4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
