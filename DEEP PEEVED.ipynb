{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPLY PEEVED: Neural Nets for Volcano Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cb6ba65d1df6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mml_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs, GetTimeToEruption, GetTimeSinceEruption, GetEQRates\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from sklearn import ensemble as ml_models\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "\n",
    "\n",
    "class BaseEarthquakes(data.Dataset):\n",
    "    \"\"\"Earthquake and Eruption Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, root, eruption_csv_path, eq_csv_path, split):\n",
    "        self.root  = root\n",
    "        self.split = split\n",
    "        self.eruption_csv_path = eruption_csv_path\n",
    "        self.eq_csv_path = eq_csv_path\n",
    "        self._get_east_rift_zone()\n",
    "        self._load_data()\n",
    "\n",
    "        self._normalize()\n",
    "        \n",
    "    def _get_east_rift_zone(self): \n",
    "        self.p = PuuOo(self.eruption_csv_path)\n",
    "        time, lat, lon, depth, mag = load_puuoo_eqs(self.eq_csv_path)\n",
    "        \n",
    "        latpts = np.array([19.3,19.5])\n",
    "        lonpts = np.array([-155.5,-155])\n",
    "        A      = np.array([lonpts,[1,1]]).T\n",
    "        line   = np.linalg.solve(A, latpts)   \n",
    "        croplocs = lat - line[0]*lon - line[1]        \n",
    "        idx = [j for j in range(len(lon)) if croplocs[j]<=0]\n",
    "        \n",
    "        self.time = np.array(time)[idx]\n",
    "        self.lat = np.array(lat)[idx]\n",
    "        self.lon = np.array(lon)[idx]\n",
    "        self.depth = np.array(depth)[idx]\n",
    "        self.mag = np.array(mag)[idx]\n",
    "        \n",
    "        print(len(idx))\n",
    "    \n",
    "    def _normalize(self):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.x)\n",
    "        self.x = scaler.transform(self.x)\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Create data list via train, val split\n",
    "\n",
    "        if self.split in [\"train\", \"val\", \"test\"]:\n",
    "            random.seed(0)\n",
    "            percent_train = 0.7 \n",
    "            percent_dev   = 0.2 # Percent test is what is left\n",
    "            \n",
    "            # Make additional array for erupting or not\n",
    "            erupt = np.array([self.p.was_erupting(t) for t in self.time])\n",
    "            \n",
    "            # Get indices of eruption and non-eruption earthquakes so we can split both\n",
    "            eruption_idx    = [i for i, e in enumerate(erupt) if e == True]\n",
    "            no_eruption_idx = [i for i, e in enumerate(erupt) if e == False]\n",
    "\n",
    "            num_train_eruptions = int(percent_train * len(eruption_idx))\n",
    "            num_val_eruptions   = int(percent_dev * len(eruption_idx))\n",
    "            num_test_eruptions  = len(eruption_idx) - num_val_eruptions - num_train_eruptions\n",
    "\n",
    "            num_train_no_eruptions = int(percent_train * len(no_eruption_idx))\n",
    "            num_val_no_eruptions   = int(percent_dev * len(no_eruption_idx))\n",
    "            num_test_no_eruptions  = len(no_eruption_idx) - num_val_no_eruptions - num_train_no_eruptions\n",
    "            \n",
    "            train_idx = sorted(random.sample(eruption_idx, num_train_eruptions))\n",
    "            remaining = sorted(list(set(eruption_idx) - set(train_idx)))\n",
    "            val_idx   = sorted(random.sample(remaining, num_val_eruptions))\n",
    "            test_idx  = sorted(list(set(remaining) - set(val_idx)))\n",
    "            \n",
    "            train_idx += sorted(random.sample(no_eruption_idx, num_train_no_eruptions))\n",
    "            remaining  = sorted(list(set(no_eruption_idx) - set(train_idx)))\n",
    "            val_idx   += sorted(random.sample(remaining, num_val_no_eruptions))\n",
    "            test_idx  += sorted(list(set(remaining) - set(val_idx)))\n",
    "            \n",
    "            print(len(train_idx))\n",
    "            print(len(val_idx))\n",
    "            print(len(test_idx))\n",
    "            \n",
    "            assert(len(train_idx) + len(val_idx) + len(test_idx) == len(erupt))\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                idx = train_idx\n",
    "            elif self.split == \"val\":\n",
    "                idx = val_idx\n",
    "            elif self.split == \"test\":\n",
    "                idx = test_idx\n",
    "            \n",
    "            # Shuffle for data loader\n",
    "            \n",
    "            random.shuffle(idx)\n",
    "            self.idx = idx\n",
    "            \n",
    "            self.time = self.time[idx]\n",
    "            self.lat = self.lat[idx]\n",
    "            self.lon = self.lon[idx]\n",
    "            self.depth = self.depth[idx]\n",
    "            self.mag = self.mag[idx]\n",
    "            self.erupt = erupt[idx]\n",
    "            \n",
    "            self.y = self.erupt\n",
    "            self.x = np.array([self.lat, self.lon, \\\n",
    "                               self.depth, self.mag]).T\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid split name: {}\".format(self.split))\n",
    "\n",
    "    def _get_label_weights(self):\n",
    "        # Get weights for a given dataset\n",
    "        num_erupt = np.sum(self.y)\n",
    "        total = len(self.y)\n",
    "        weights = [1, total/num_erupt]\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.erupt)\n",
    "\n",
    "class NoDerivedFeatures(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoDerivedFeatures, self).__init__(**kwargs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "class ExtraFeatures(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(ExtraFeatures, self).__init__(**kwargs)\n",
    "        self.derive_extra_features()\n",
    "        self._normalize()\n",
    "    \n",
    "    def derive_extra_features(self):\n",
    "        self.SecsToEruption = GetTimeToEruption(self.time, self.p)\n",
    "        self.SecsSinceEruption = GetTimeSinceEruption(self.time, self.p)\n",
    "        self.EQsLastDay, self.EQsLastWeek, self.EQsLastMonth = GetEQRates(self.time)\n",
    "        self.x = np.hstack((self.x, self.EQsLastMonth[:,np.newaxis]))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruption_csv_path = 'PuuOo.csv'\n",
    "eq_csv_path       = 'puuoo_earthquakes.csv' \n",
    "\n",
    "dataset_train = ExtraFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "dataset_val = ExtraFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "dataset_test = ExtraFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"test\",\n",
    "    )\n",
    "\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=50)\n",
    "loader_val = DataLoader(dataset_val, batch_size=50)\n",
    "loader_test = DataLoader(dataset_test, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dataset_train.lon, dataset_train.lat, 'b.', label='training')\n",
    "plt.plot(dataset_val.lon, dataset_val.lat, 'r^', label='dev')\n",
    "plt.plot(dataset_test.lon, dataset_test.lat, 'gx', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dataset_train.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('train.txt', sorted(dataset_train.idx), '%d')\n",
    "np.savetxt('dev.txt', sorted(dataset_val.idx), '%d')\n",
    "np.savetxt('test.txt', sorted(dataset_test.idx), '%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_layer_model(input_features, hidden_layer_sizes=[1000,500], output_size=1):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1],output_size)\n",
    "    )\n",
    "    \n",
    "    return model.double()\n",
    "\n",
    "def get_four_layer_model(input_features, hidden_layer_sizes=[1000,1000,1000,1000], output_size=1):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[2], hidden_layer_sizes[3]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[3],output_size),\n",
    "    )\n",
    "    \n",
    "    return model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_four_layer_model(4)\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    num_pos     = 0\n",
    "    predicted   = []\n",
    "    val_label   = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = torch.nn.functional.sigmoid(model(x))\n",
    "            \n",
    "            preds = torch.squeeze((scores > 0.5).long())\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            num_pos += (preds).sum()\n",
    "            \n",
    "            predicted += preds.tolist()\n",
    "            val_label += y.tolist()\n",
    "            \n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Accuracy on ' + loader.dataset.split + ': Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        print(metrics.confusion_matrix(val_label, predicted))\n",
    "        print('Kappa:', metrics.cohen_kappa_score(val_label, predicted))\n",
    "        print('AUROC:', metrics.roc_auc_score(val_label, predicted))\n",
    "        print('Num pos: %d' % num_pos)\n",
    "        \n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    \n",
    "    weights = loader_train.dataset._get_label_weights()\n",
    "    weights = torch.tensor(weights)\n",
    "    print('Weghts are: ', weights)\n",
    "    #criterion = nn.CrossEntropyLoss(weight=weights.double())\n",
    "    #criterion = nn.BCEWithLogitsLoss(pos_weight=weights[1])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            y = torch.unsqueeze(y,1)\n",
    "           \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "                    \n",
    "            scores = model(x)\n",
    "            loss = criterion(scores.float(), y.float())\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Epoch %d, loss = %.4f' % (e, loss.item()))\n",
    "            check_accuracy(loader_train, model)\n",
    "            check_accuracy(loader_val, model)\n",
    "            check_accuracy(loader_test, model)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weghts are:  tensor([1.0000, 4.4627])\n",
      "Epoch 0, loss = 0.5598\n",
      "Accuracy on train: Got 2160 / 2633 correct (82.04)\n",
      "[[2024   19]\n",
      " [ 454  136]]\n",
      "Kappa: 0.29981475078499\n",
      "AUROC: 0.6106042128143225\n",
      "Num pos: 155\n",
      "Accuracy on val: Got 603 / 752 correct (80.19)\n",
      "[[577   7]\n",
      " [142  26]]\n",
      "Kappa: 0.200022846698652\n",
      "AUROC: 0.5713878016960209\n",
      "Num pos: 33\n",
      "Accuracy on test: Got 306 / 379 correct (80.74)\n",
      "[[292   1]\n",
      " [ 72  14]]\n",
      "Kappa: 0.22499229670298881\n",
      "AUROC: 0.5796888641955711\n",
      "Num pos: 15\n",
      "\n",
      "Epoch 150, loss = 0.5135\n",
      "Accuracy on train: Got 2201 / 2633 correct (83.59)\n",
      "[[2022   21]\n",
      " [ 411  179]]\n",
      "Kappa: 0.3831817664188454\n",
      "AUROC: 0.646555414520023\n",
      "Num pos: 200\n",
      "Accuracy on val: Got 603 / 752 correct (80.19)\n",
      "[[560  24]\n",
      " [125  43]]\n",
      "Kappa: 0.2733969703257938\n",
      "AUROC: 0.607428245270711\n",
      "Num pos: 67\n",
      "Accuracy on test: Got 312 / 379 correct (82.32)\n",
      "[[288   5]\n",
      " [ 62  24]]\n",
      "Kappa: 0.34209912687514565\n",
      "AUROC: 0.6310024605127391\n",
      "Num pos: 29\n",
      "\n",
      "Epoch 300, loss = 0.4966\n",
      "Accuracy on train: Got 2202 / 2633 correct (83.63)\n",
      "[[2019   24]\n",
      " [ 407  183]]\n",
      "Kappa: 0.38798516481945977\n",
      "AUROC: 0.6492110306378954\n",
      "Num pos: 207\n",
      "Accuracy on val: Got 599 / 752 correct (79.65)\n",
      "[[556  28]\n",
      " [125  43]]\n",
      "Kappa: 0.26185588174912744\n",
      "AUROC: 0.6040035877364645\n",
      "Num pos: 71\n",
      "Accuracy on test: Got 311 / 379 correct (82.06)\n",
      "[[287   6]\n",
      " [ 62  24]]\n",
      "Kappa: 0.33584166580764874\n",
      "AUROC: 0.6292959758711009\n",
      "Num pos: 30\n",
      "\n",
      "Epoch 450, loss = 0.4890\n",
      "Accuracy on train: Got 2201 / 2633 correct (83.59)\n",
      "[[2017   26]\n",
      " [ 406  184]]\n",
      "Kappa: 0.3880038738835683\n",
      "AUROC: 0.6495690120046127\n",
      "Num pos: 210\n",
      "Accuracy on val: Got 598 / 752 correct (79.52)\n",
      "[[555  29]\n",
      " [125  43]]\n",
      "Kappa: 0.25900900900900903\n",
      "AUROC: 0.6031474233529028\n",
      "Num pos: 72\n",
      "Accuracy on test: Got 312 / 379 correct (82.32)\n",
      "[[287   6]\n",
      " [ 61  25]]\n",
      "Kappa: 0.34908102842787925\n",
      "AUROC: 0.635109929359473\n",
      "Num pos: 31\n",
      "\n",
      "Epoch 600, loss = 0.4920\n",
      "Accuracy on train: Got 2204 / 2633 correct (83.71)\n",
      "[[2019   24]\n",
      " [ 405  185]]\n",
      "Kappa: 0.391778356802127\n",
      "AUROC: 0.6509059458921327\n",
      "Num pos: 209\n",
      "Accuracy on val: Got 598 / 752 correct (79.52)\n",
      "[[555  29]\n",
      " [125  43]]\n",
      "Kappa: 0.25900900900900903\n",
      "AUROC: 0.6031474233529028\n",
      "Num pos: 72\n",
      "Accuracy on test: Got 312 / 379 correct (82.32)\n",
      "[[287   6]\n",
      " [ 61  25]]\n",
      "Kappa: 0.34908102842787925\n",
      "AUROC: 0.635109929359473\n",
      "Num pos: 31\n",
      "\n",
      "Epoch 750, loss = 0.4936\n",
      "Accuracy on train: Got 2204 / 2633 correct (83.71)\n",
      "[[2019   24]\n",
      " [ 405  185]]\n",
      "Kappa: 0.391778356802127\n",
      "AUROC: 0.6509059458921327\n",
      "Num pos: 209\n",
      "Accuracy on val: Got 599 / 752 correct (79.65)\n",
      "[[556  28]\n",
      " [125  43]]\n",
      "Kappa: 0.26185588174912744\n",
      "AUROC: 0.6040035877364645\n",
      "Num pos: 71\n",
      "Accuracy on test: Got 312 / 379 correct (82.32)\n",
      "[[287   6]\n",
      " [ 61  25]]\n",
      "Kappa: 0.34908102842787925\n",
      "AUROC: 0.635109929359473\n",
      "Num pos: 31\n",
      "\n",
      "Epoch 900, loss = 0.4902\n",
      "Accuracy on train: Got 2205 / 2633 correct (83.74)\n",
      "[[2020   23]\n",
      " [ 405  185]]\n",
      "Kappa: 0.3927209981818124\n",
      "AUROC: 0.6511506840223334\n",
      "Num pos: 208\n",
      "Accuracy on val: Got 598 / 752 correct (79.52)\n",
      "[[555  29]\n",
      " [125  43]]\n",
      "Kappa: 0.25900900900900903\n",
      "AUROC: 0.6031474233529028\n",
      "Num pos: 72\n",
      "Accuracy on test: Got 312 / 379 correct (82.32)\n",
      "[[287   6]\n",
      " [ 61  25]]\n",
      "Kappa: 0.34908102842787925\n",
      "AUROC: 0.635109929359473\n",
      "Num pos: 31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "weight_decay  = 0.001\n",
    "print_every = 150\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float64\n",
    "\n",
    "#two_layer_model = get_two_layer_model(5)\n",
    "optimizer = optim.SGD(two_layer_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "train_model(two_layer_model, optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n",
      "0.8089368258859785\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(1-loader_train.dataset.y)/len(loader_train.dataset.y))\n",
    "print(np.sum(1-loader_val.dataset.y)/len(loader_val.dataset.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weghts are:  tensor([1.0000, 4.4627])\n",
      "Epoch 0, loss = 0.5829\n",
      "Accuracy on train: Got 2043 / 2633 correct (77.59)\n",
      "[[2043    0]\n",
      " [ 590    0]]\n",
      "Kappa: 0.0\n",
      "AUROC: 0.5\n",
      "Num pos: 0\n",
      "Accuracy on val: Got 584 / 752 correct (77.66)\n",
      "[[584   0]\n",
      " [168   0]]\n",
      "Kappa: 0.0\n",
      "AUROC: 0.5\n",
      "Num pos: 0\n",
      "Accuracy on test: Got 293 / 379 correct (77.31)\n",
      "[[293   0]\n",
      " [ 86   0]]\n",
      "Kappa: 0.0\n",
      "AUROC: 0.5\n",
      "Num pos: 0\n",
      "\n",
      "Epoch 50, loss = 0.5559\n",
      "Accuracy on train: Got 2171 / 2633 correct (82.45)\n",
      "[[2025   18]\n",
      " [ 444  146]]\n",
      "Kappa: 0.3210895197018354\n",
      "AUROC: 0.6193235272157098\n",
      "Num pos: 164\n",
      "Accuracy on val: Got 608 / 752 correct (80.85)\n",
      "[[576   8]\n",
      " [136  32]]\n",
      "Kappa: 0.24261414503133405\n",
      "AUROC: 0.588388780169602\n",
      "Num pos: 40\n",
      "Accuracy on test: Got 304 / 379 correct (80.21)\n",
      "[[290   3]\n",
      " [ 72  14]]\n",
      "Kappa: 0.2128873258937224\n",
      "AUROC: 0.5762758949122947\n",
      "Num pos: 17\n",
      "\n",
      "Epoch 100, loss = 0.5407\n",
      "Accuracy on train: Got 2177 / 2633 correct (82.68)\n",
      "[[2027   16]\n",
      " [ 440  150]]\n",
      "Kappa: 0.33099158172987986\n",
      "AUROC: 0.6232028339845856\n",
      "Num pos: 166\n",
      "Accuracy on val: Got 606 / 752 correct (80.59)\n",
      "[[574  10]\n",
      " [136  32]]\n",
      "Kappa: 0.2365376056964843\n",
      "AUROC: 0.5866764514024788\n",
      "Num pos: 42\n",
      "Accuracy on test: Got 308 / 379 correct (81.27)\n",
      "[[290   3]\n",
      " [ 68  18]]\n",
      "Kappa: 0.27156817628109686\n",
      "AUROC: 0.599531708865783\n",
      "Num pos: 21\n",
      "\n",
      "Epoch 150, loss = 0.5364\n",
      "Accuracy on train: Got 2183 / 2633 correct (82.91)\n",
      "[[2028   15]\n",
      " [ 435  155]]\n",
      "Kappa: 0.34192548653692356\n",
      "AUROC: 0.6276848602503795\n",
      "Num pos: 170\n",
      "Accuracy on val: Got 608 / 752 correct (80.85)\n",
      "[[574  10]\n",
      " [134  34]]\n",
      "Kappa: 0.25132743362831855\n",
      "AUROC: 0.5926288323548597\n",
      "Num pos: 44\n",
      "Accuracy on test: Got 309 / 379 correct (81.53)\n",
      "[[290   3]\n",
      " [ 67  19]]\n",
      "Kappa: 0.28582965435555086\n",
      "AUROC: 0.605345662354155\n",
      "Num pos: 22\n",
      "\n",
      "Epoch 200, loss = 0.5366\n",
      "Accuracy on train: Got 2183 / 2633 correct (82.91)\n",
      "[[2028   15]\n",
      " [ 435  155]]\n",
      "Kappa: 0.34192548653692356\n",
      "AUROC: 0.6276848602503795\n",
      "Num pos: 170\n",
      "Accuracy on val: Got 609 / 752 correct (80.98)\n",
      "[[573  11]\n",
      " [132  36]]\n",
      "Kappa: 0.2628865979381443\n",
      "AUROC: 0.5977250489236791\n",
      "Num pos: 47\n",
      "Accuracy on test: Got 309 / 379 correct (81.53)\n",
      "[[290   3]\n",
      " [ 67  19]]\n",
      "Kappa: 0.28582965435555086\n",
      "AUROC: 0.605345662354155\n",
      "Num pos: 22\n",
      "\n",
      "Epoch 250, loss = 0.5338\n",
      "Accuracy on train: Got 2185 / 2633 correct (82.99)\n",
      "[[2026   17]\n",
      " [ 431  159]]\n",
      "Kappa: 0.34800723856648075\n",
      "AUROC: 0.6305852144984527\n",
      "Num pos: 176\n",
      "Accuracy on val: Got 609 / 752 correct (80.98)\n",
      "[[573  11]\n",
      " [132  36]]\n",
      "Kappa: 0.2628865979381443\n",
      "AUROC: 0.5977250489236791\n",
      "Num pos: 47\n",
      "Accuracy on test: Got 309 / 379 correct (81.53)\n",
      "[[290   3]\n",
      " [ 67  19]]\n",
      "Kappa: 0.28582965435555086\n",
      "AUROC: 0.605345662354155\n",
      "Num pos: 22\n",
      "\n",
      "Epoch 300, loss = 0.5315\n",
      "Accuracy on train: Got 2186 / 2633 correct (83.02)\n",
      "[[2025   18]\n",
      " [ 429  161]]\n",
      "Kappa: 0.35102618776250205\n",
      "AUROC: 0.6320353916224895\n",
      "Num pos: 179\n",
      "Accuracy on val: Got 609 / 752 correct (80.98)\n",
      "[[572  12]\n",
      " [131  37]]\n",
      "Kappa: 0.2670665212649945\n",
      "AUROC: 0.599845075016308\n",
      "Num pos: 49\n",
      "Accuracy on test: Got 309 / 379 correct (81.53)\n",
      "[[290   3]\n",
      " [ 67  19]]\n",
      "Kappa: 0.28582965435555086\n",
      "AUROC: 0.605345662354155\n",
      "Num pos: 22\n",
      "\n",
      "Epoch 350, loss = 0.5293\n",
      "Accuracy on train: Got 2189 / 2633 correct (83.14)\n",
      "[[2026   17]\n",
      " [ 427  163]]\n",
      "Kappa: 0.3558977636486852\n",
      "AUROC: 0.6339750450069273\n",
      "Num pos: 180\n",
      "Accuracy on val: Got 607 / 752 correct (80.72)\n",
      "[[570  14]\n",
      " [131  37]]\n",
      "Kappa: 0.26100628930817615\n",
      "AUROC: 0.5981327462491847\n",
      "Num pos: 51\n",
      "Accuracy on test: Got 309 / 379 correct (81.53)\n",
      "[[290   3]\n",
      " [ 67  19]]\n",
      "Kappa: 0.28582965435555086\n",
      "AUROC: 0.605345662354155\n",
      "Num pos: 22\n",
      "\n",
      "Epoch 400, loss = 0.5275\n",
      "Accuracy on train: Got 2191 / 2633 correct (83.21)\n",
      "[[2026   17]\n",
      " [ 425  165]]\n",
      "Kappa: 0.3598241062843387\n",
      "AUROC: 0.6356699602611645\n",
      "Num pos: 182\n",
      "Accuracy on val: Got 607 / 752 correct (80.72)\n",
      "[[570  14]\n",
      " [131  37]]\n",
      "Kappa: 0.26100628930817615\n",
      "AUROC: 0.5981327462491847\n",
      "Num pos: 51\n",
      "Accuracy on test: Got 310 / 379 correct (81.79)\n",
      "[[290   3]\n",
      " [ 66  20]]\n",
      "Kappa: 0.2999330745549458\n",
      "AUROC: 0.6111596158425271\n",
      "Num pos: 23\n",
      "\n",
      "Epoch 450, loss = 0.5265\n",
      "Accuracy on train: Got 2191 / 2633 correct (83.21)\n",
      "[[2025   18]\n",
      " [ 424  166]]\n",
      "Kappa: 0.3608458157908899\n",
      "AUROC: 0.6362726797580825\n",
      "Num pos: 184\n",
      "Accuracy on val: Got 607 / 752 correct (80.72)\n",
      "[[570  14]\n",
      " [131  37]]\n",
      "Kappa: 0.26100628930817615\n",
      "AUROC: 0.5981327462491847\n",
      "Num pos: 51\n",
      "Accuracy on test: Got 310 / 379 correct (81.79)\n",
      "[[290   3]\n",
      " [ 66  20]]\n",
      "Kappa: 0.2999330745549458\n",
      "AUROC: 0.6111596158425271\n",
      "Num pos: 23\n",
      "\n",
      "Epoch 500, loss = 0.5253\n",
      "Accuracy on train: Got 2193 / 2633 correct (83.29)\n",
      "[[2025   18]\n",
      " [ 422  168]]\n",
      "Kappa: 0.3647517612275515\n",
      "AUROC: 0.6379675950123199\n",
      "Num pos: 186\n",
      "Accuracy on val: Got 607 / 752 correct (80.72)\n",
      "[[570  14]\n",
      " [131  37]]\n",
      "Kappa: 0.26100628930817615\n",
      "AUROC: 0.5981327462491847\n",
      "Num pos: 51\n",
      "Accuracy on test: Got 310 / 379 correct (81.79)\n",
      "[[290   3]\n",
      " [ 66  20]]\n",
      "Kappa: 0.2999330745549458\n",
      "AUROC: 0.6111596158425271\n",
      "Num pos: 23\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4ea0f5ac0252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfour_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfour_layer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-b224d3da71c6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.02\n",
    "weight_decay  = 0.01\n",
    "print_every = 50\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float64\n",
    "\n",
    "four_layer_model = get_four_layer_model(5)\n",
    "optimizer = optim.SGD(four_layer_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "train_model(four_layer_model, optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEarthquakes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7cd3cd2029ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTimeDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEarthquakes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderive_extra_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEarthquakes' is not defined"
     ]
    }
   ],
   "source": [
    "class TimeDS(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TimeDS, self).__init__(**kwargs)\n",
    "        self.derive_extra_features()\n",
    "        self._normalize()\n",
    "    \n",
    "    def derive_extra_features(self):\n",
    "        self.SecsToEruption = GetTimeToEruption(self.time, self.p)\n",
    "        self.SecsSinceEruption = GetTimeSinceEruption(self.time, self.p)\n",
    "        self.EQsLastDay, self.EQsLastWeek, self.EQsLastMonth = GetEQRates(self.time)\n",
    "        self.x = np.hstack((self.x, self.EQsLastMonth[:,np.newaxis]))\n",
    "        self.y = self.SecsToEruption\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_layer_time_model(input_features, hidden_layer_sizes=[1000,500], output_size=1):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1],output_size),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    return model.double()\n",
    "\n",
    "def get_four_layer_time_model(input_features, hidden_layer_sizes=[1000,1000,1000,1000], output_size=1):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[2], hidden_layer_sizes[3]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[3],output_size),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    return model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, criterion):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    num_pos     = 0\n",
    "    predicted   = []\n",
    "    val_label   = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for x, y in loader:\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            num_batch = x.shape[0]\n",
    "            num_total += num_batch\n",
    "            \n",
    "            scores = torch.squeeze(model(x)).long()\n",
    "            loss = criterion(scores.float(), y.float())\n",
    "            \n",
    "            total_loss += loss * num_batch         \n",
    "            \n",
    "        average_loss = total_loss/num_total\n",
    "        print(('Accuracy on ' + loader.dataset.split + \": %.2f\") % average_loss)\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    criterion.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            y = torch.unsqueeze(y,1)\n",
    "           \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "                    \n",
    "            scores = model(x)\n",
    "            loss = criterion(scores.float(), y.float())\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Epoch %d, loss = %.4f' % (e, loss.item()))\n",
    "            check_accuracy(loader_train, model, criterion)\n",
    "            check_accuracy(loader_val, model, criterion)\n",
    "            check_accuracy(loader_test, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "weight_decay  = 0.001\n",
    "print_every = 150\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float64\n",
    "\n",
    "two_layer_model = get_two_layer_time_model(5)\n",
    "optimizer = optim.SGD(two_layer_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "train_model(two_layer_model, optimizer, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
