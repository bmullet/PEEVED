{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPLY PEEVED: Neural Nets for Volcano Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from sklearn import ensemble as ml_models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from util import load_hypocenters, PuuOo, load_puuoo_eqs\n",
    "\n",
    "\n",
    "class BaseEarthquakes(data.Dataset):\n",
    "    \"\"\"Earthquake and Eruption Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, root, eruption_csv_path, eq_csv_path, split):\n",
    "        self.root  = root\n",
    "        self.split = split\n",
    "        self.eruption_csv_path = eruption_csv_path\n",
    "        self.eq_csv_path = eq_csv_path\n",
    "        self._load_data()\n",
    "    \n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Create data list via train, val split\n",
    "        p = PuuOo(eruption_csv_path)\n",
    "        time, lat, lon, depth, mag = load_puuoo_eqs(eq_csv_path)\n",
    "        \n",
    "        if self.split in [\"train\", \"val\"]:\n",
    "            random.seed(0)\n",
    "            percent_train = 0.8 \n",
    "            \n",
    "            # Make additional array for erupting or not\n",
    "            erupt = np.array([p.was_erupting(t) for t in time])\n",
    "            \n",
    "            # Get indices of eruption and non-eruption earthquakes so we can split both\n",
    "            eruption_idx    = [i for i, e in enumerate(erupt) if e == True]\n",
    "            no_eruption_idx = [i for i, e in enumerate(erupt) if e == False]\n",
    "\n",
    "            num_train_eruptions = int(percent_train * len(eruption_idx))\n",
    "            num_val_eruptions   = len(eruption_idx) - num_train_eruptions\n",
    "\n",
    "            num_train_no_eruptions = int(percent_train * len(no_eruption_idx))\n",
    "            num_val_no_eruptions   = len(no_eruption_idx) - num_train_eruptions\n",
    "\n",
    "            train_idx = sorted(random.sample(eruption_idx, num_train_eruptions))\n",
    "            val_idx   = sorted(list(set(eruption_idx) - set(train_idx)))\n",
    "            train_idx += sorted(random.sample(no_eruption_idx, num_train_no_eruptions))\n",
    "            val_idx   += sorted(list(set(no_eruption_idx) - set(train_idx)))\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                idx = train_idx\n",
    "            elif self.split == \"val\":\n",
    "                idx = val_idx\n",
    "            \n",
    "            # Shuffle for data loader\n",
    "            random.shuffle(idx)\n",
    "            \n",
    "            self.time = np.array(time)[idx]\n",
    "            self.lat = np.array(lat)[idx]\n",
    "            self.lon = np.array(lon)[idx]\n",
    "            self.depth = np.array(depth)[idx]\n",
    "            self.mag = np.array(mag)[idx]\n",
    "            self.erupt = np.array(erupt)[idx]\n",
    "                   \n",
    "        else:\n",
    "            raise ValueError(\"Invalid split name: {}\".format(self.split))\n",
    "\n",
    "    def _get_label_weights(self):\n",
    "        # Get weights for a given dataset\n",
    "        ids = [self._get_disaster_id(sample_name) for sample_name in self.files]\n",
    "        id_counts = Counter(ids)\n",
    "\n",
    "        total = len(ids)\n",
    "\n",
    "        weights = [min(total/id_counts[i], 15) for i in range(self.num_classes)]\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.erupt)\n",
    "\n",
    "\n",
    "class NoDerivedFeatures(BaseEarthquakes):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoDerivedFeatures, self).__init__(**kwargs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        y = self.erupt[index]\n",
    "        x = np.array([self.lat[index], self.lon[index], self.depth[index], self.mag[index]])\n",
    "        return x,y\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruption_csv_path = 'PuuOo.csv'\n",
    "eq_csv_path       = 'puuoo_earthquakes.csv' \n",
    "\n",
    "dataset_train = NoDerivedFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "dataset_val = NoDerivedFeatures(\n",
    "        root=\".\",\n",
    "        eruption_csv_path=eruption_csv_path, \n",
    "        eq_csv_path=eq_csv_path,    \n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=50)\n",
    "loader_val = DataLoader(dataset_val, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_layer_model(input_features, hidden_layer_sizes=[5000,5000], output_size=2):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1],output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return model.double()\n",
    "\n",
    "def get_four_layer_model(input_features, hidden_layer_sizes=[1000,1000,1000,1000], output_size=2):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, hidden_layer_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[2], hidden_layer_sizes[3]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_sizes[3],output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of Sequential(\n",
      "  (0): Linear(in_features=4, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  (9): Sigmoid()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = get_four_layer_model(4)\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.split == \"train\":\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Epoch %d, loss = %.4f' % (e, loss.item()))\n",
    "            check_accuracy(loader_val, model)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 0.3860\n",
      "Checking accuracy on validation set\n",
      "Got 4512 / 5116 correct (88.19)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-50e35fb4da5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-6ef8c6dbe384>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# computed by the backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs229/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000000\n",
    "print_every = 10\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "\n",
    "model = get_two_layer_model(4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
